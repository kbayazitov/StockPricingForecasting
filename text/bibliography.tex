\newpage

\begin{thebibliography}{99}

    \bibitem{ARIMA} 
    \textit{Лукашин Ю.П.} Адаптивные методы краткосрочного прогнозирования временных рядов. Финансы и статистика, 2003

    \bibitem{Stock price ARIMA}
    \textit{Ariyo A.A.,Adewumi A.O.} Stock price prediction using the ARIMA model, 2014

    \bibitem{Критерии стационарности 1}
    \textit{Patterson K.} Unit Root Tests In Time Series Volume 1, 2011

    \bibitem{Критерии стационарности 2}
    \textit{Herranz E.} Unit root tests, 2017

    \bibitem{LSTM1}
    \textit{Hochreiter S., Schmidhuber J.} Neural Computation 9(8), 1997

    \bibitem{LSTM2}
    \textit{Greff K., Schmidhuber J.} LSTM: A Search Space Odyssey, 2017

    \bibitem{LSTM3}
    \textit{Christopher Olah} Understanding LSTM Networks, 2015 \\
    https://colah.github.io/posts/2015-08-Understanding-LSTMs

    \bibitem{EncoderDecoder} 
    \textit{Cho K.} On the Properties of Neural Machine Translation: Encoder–Decoder Approaches, 2014

    \bibitem{RNN Attention}
    \textit{Bahdanau D.} Neural Machine Translation By Jointly Learning To Align And Translate, 2016

    \bibitem{Attention Mechanisms}
    \textit{Dichao Hu} An Introductory Survey on Attention Mechanisms in NLP Problems, 2018

    \bibitem{Self-Attention}
    \textit{Lin Z., Bengio Y.} A Structured Self-Attentive Sentence Embedding, 2017

    \bibitem{Attention is all you need}
    \textit{Vaswani A. et al.} Attention Is All You Need, 2017

    \bibitem{Positional Encoding}
    \textit{Shaw P., Uszkoreit J., Vaswani A.} Self-Attention with Relative Position Representations, 2018

    \bibitem{Adam}
    \textit{Kingma D. P., Ba J.} Adam: A Method for Stochastic Optimization, 2017

    \bibitem{Transformer Time Series}
    \textit{Wu N., Green B.} Deep Transformer Models for Time Series Forecasting, 2020

    \bibitem{LogTrans}
    \textit{Li S., Jin X.} Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting, 2019

    \bibitem{Autoformer}
    \textit{Wu H., Xu J.} Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting, 2021

    \bibitem{Informer}
    \textit{Zhou H., Zhang S.} Informer: Beyond efficient transformer for long sequence time-series forecasting, 2021

    \bibitem{Pyraformer}
    \textit{Liu S., Yu H.} Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting, 2021

    \bibitem{FEDformer}
    \textit{Zhou T., Ma Z.} FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting, 2022
    
    \bibitem{iTransformer}
    \textit{Liu Y., Hu T., Zhang H.} iTransformer: Inverted Transformers Are Effective for Time Series Forecasting, 2023

    \bibitem{Лекции}
    \textit{Воронцов К. В.} Машинное обучение, курс лекций
    
    \bibitem{Github}
    \textit{Отчет} https://github.com/kbayazitov/StockPricingForecasting
	
\end{thebibliography}